{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc29931-57d0-4717-a955-d361ea1676a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (20801, 40)\n",
      "Optimized Filtering Parameters:\n",
      " Process Variance: 0.0006715429033260829\n",
      " Measurement Variance: 0.05235353908705457\n",
      " SG Window Length: 51\n",
      " SG Polyorder: 1\n",
      "Filtering parameters saved to filtering_params.json\n",
      "Processed data shape: (19718, 54)\n",
      "Extracted features shape: (787, 17)\n",
      "Sequence shape: (778, 10, 13) Labels shape: (787, 17)\n",
      "Training Hybrid CNN-RNN Model...\n",
      "Epoch 1/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.5740 - loss: 0.6884 - precision: 0.1807 - recall: 0.4988 - val_accuracy: 0.5641 - val_loss: 0.6833 - val_precision: 0.2872 - val_recall: 0.9643\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6101 - loss: 0.6065 - precision: 0.2811 - recall: 0.7443 - val_accuracy: 0.5897 - val_loss: 0.6667 - val_precision: 0.2955 - val_recall: 0.9286\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6981 - loss: 0.5392 - precision: 0.4033 - recall: 0.8408 - val_accuracy: 0.6667 - val_loss: 0.5839 - val_precision: 0.3235 - val_recall: 0.7857\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7180 - loss: 0.5082 - precision: 0.4003 - recall: 0.7713 - val_accuracy: 0.6603 - val_loss: 0.5824 - val_precision: 0.3288 - val_recall: 0.8571\n",
      "Epoch 5/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6604 - loss: 0.4921 - precision: 0.3334 - recall: 0.9082 - val_accuracy: 0.7628 - val_loss: 0.4352 - val_precision: 0.4082 - val_recall: 0.7143\n",
      "Epoch 6/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7488 - loss: 0.4200 - precision: 0.3908 - recall: 0.8001 - val_accuracy: 0.7051 - val_loss: 0.4884 - val_precision: 0.3676 - val_recall: 0.8929\n",
      "Epoch 7/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7297 - loss: 0.4608 - precision: 0.4352 - recall: 0.8799 - val_accuracy: 0.7436 - val_loss: 0.4535 - val_precision: 0.3966 - val_recall: 0.8214\n",
      "Epoch 8/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7362 - loss: 0.3952 - precision: 0.4096 - recall: 0.9073 - val_accuracy: 0.7628 - val_loss: 0.4334 - val_precision: 0.4082 - val_recall: 0.7143\n",
      "Epoch 9/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7855 - loss: 0.3668 - precision: 0.4954 - recall: 0.8978 - val_accuracy: 0.7179 - val_loss: 0.4576 - val_precision: 0.3750 - val_recall: 0.8571\n",
      "Epoch 10/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8050 - loss: 0.3132 - precision: 0.5033 - recall: 0.9349 - val_accuracy: 0.8590 - val_loss: 0.2828 - val_precision: 0.6071 - val_recall: 0.6071\n",
      "Epoch 11/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8396 - loss: 0.3427 - precision: 0.5490 - recall: 0.8221 - val_accuracy: 0.8269 - val_loss: 0.3341 - val_precision: 0.5111 - val_recall: 0.8214\n",
      "Epoch 12/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8641 - loss: 0.2427 - precision: 0.5716 - recall: 0.9673 - val_accuracy: 0.7821 - val_loss: 0.4040 - val_precision: 0.4423 - val_recall: 0.8214\n",
      "Epoch 13/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8621 - loss: 0.2429 - precision: 0.5892 - recall: 0.9638 - val_accuracy: 0.8910 - val_loss: 0.2671 - val_precision: 0.6571 - val_recall: 0.8214\n",
      "Epoch 14/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8773 - loss: 0.2347 - precision: 0.6267 - recall: 0.9371 - val_accuracy: 0.8910 - val_loss: 0.2744 - val_precision: 0.6897 - val_recall: 0.7143\n",
      "Epoch 15/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9249 - loss: 0.1867 - precision: 0.7317 - recall: 0.9685 - val_accuracy: 0.8397 - val_loss: 0.3236 - val_precision: 0.5319 - val_recall: 0.8929\n",
      "Epoch 16/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8453 - loss: 0.2750 - precision: 0.5640 - recall: 0.9691 - val_accuracy: 0.8590 - val_loss: 0.3414 - val_precision: 0.5789 - val_recall: 0.7857\n",
      "Epoch 17/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9178 - loss: 0.1718 - precision: 0.7276 - recall: 0.9424 - val_accuracy: 0.8397 - val_loss: 0.3759 - val_precision: 0.5349 - val_recall: 0.8214\n",
      "Epoch 18/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9294 - loss: 0.1878 - precision: 0.7723 - recall: 0.9250 - val_accuracy: 0.9167 - val_loss: 0.2717 - val_precision: 0.8000 - val_recall: 0.7143\n",
      "Epoch 19/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9149 - loss: 0.2077 - precision: 0.7327 - recall: 0.8947 - val_accuracy: 0.9038 - val_loss: 0.2463 - val_precision: 0.6970 - val_recall: 0.8214\n",
      "Epoch 20/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.1176 - precision: 0.7516 - recall: 0.9937 - val_accuracy: 0.8846 - val_loss: 0.2611 - val_precision: 0.6389 - val_recall: 0.8214\n",
      "Epoch 21/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8798 - loss: 0.2371 - precision: 0.5351 - recall: 0.9421 - val_accuracy: 0.8333 - val_loss: 0.3808 - val_precision: 0.5227 - val_recall: 0.8214\n",
      "Epoch 22/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8749 - loss: 0.2511 - precision: 0.5910 - recall: 0.9603 - val_accuracy: 0.8846 - val_loss: 0.2802 - val_precision: 0.6471 - val_recall: 0.7857\n",
      "Epoch 23/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9282 - loss: 0.1632 - precision: 0.7322 - recall: 0.9678 - val_accuracy: 0.8590 - val_loss: 0.3331 - val_precision: 0.5682 - val_recall: 0.8929\n",
      "Epoch 24/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9322 - loss: 0.1357 - precision: 0.7052 - recall: 0.9961 - val_accuracy: 0.8718 - val_loss: 0.2802 - val_precision: 0.6000 - val_recall: 0.8571\n",
      "Epoch 25/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9557 - loss: 0.0982 - precision: 0.8348 - recall: 0.9776 - val_accuracy: 0.8846 - val_loss: 0.2568 - val_precision: 0.6316 - val_recall: 0.8571\n",
      "Epoch 26/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9453 - loss: 0.1084 - precision: 0.7584 - recall: 0.9880 - val_accuracy: 0.9295 - val_loss: 0.2180 - val_precision: 0.7931 - val_recall: 0.8214\n",
      "Epoch 27/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9785 - loss: 0.0751 - precision: 0.9093 - recall: 0.9878 - val_accuracy: 0.9231 - val_loss: 0.2241 - val_precision: 0.7857 - val_recall: 0.7857\n",
      "Epoch 28/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9694 - loss: 0.0859 - precision: 0.8667 - recall: 0.9760 - val_accuracy: 0.9295 - val_loss: 0.2247 - val_precision: 0.7931 - val_recall: 0.8214\n",
      "Epoch 29/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0654 - precision: 0.8840 - recall: 1.0000 - val_accuracy: 0.9359 - val_loss: 0.2176 - val_precision: 0.8462 - val_recall: 0.7857\n",
      "Epoch 30/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 0.0512 - precision: 0.9349 - recall: 0.9978 - val_accuracy: 0.9487 - val_loss: 0.1997 - val_precision: 0.8571 - val_recall: 0.8571\n",
      "Epoch 31/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0468 - precision: 0.8978 - recall: 0.9992 - val_accuracy: 0.9423 - val_loss: 0.2117 - val_precision: 0.8800 - val_recall: 0.7857\n",
      "Epoch 32/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0636 - precision: 0.8860 - recall: 1.0000 - val_accuracy: 0.9295 - val_loss: 0.2231 - val_precision: 0.7742 - val_recall: 0.8571\n",
      "Epoch 33/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9666 - loss: 0.0571 - precision: 0.8387 - recall: 0.9862 - val_accuracy: 0.9359 - val_loss: 0.2452 - val_precision: 0.8000 - val_recall: 0.8571\n",
      "Epoch 34/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0580 - precision: 0.8627 - recall: 1.0000 - val_accuracy: 0.9359 - val_loss: 0.2315 - val_precision: 0.8214 - val_recall: 0.8214\n",
      "Epoch 35/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9822 - loss: 0.0465 - precision: 0.9149 - recall: 0.9983 - val_accuracy: 0.9295 - val_loss: 0.2737 - val_precision: 0.7576 - val_recall: 0.8929\n",
      "Epoch 36/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9450 - loss: 0.0773 - precision: 0.8033 - recall: 0.9899 - val_accuracy: 0.9423 - val_loss: 0.2442 - val_precision: 0.9130 - val_recall: 0.7500\n",
      "Epoch 37/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9819 - loss: 0.0411 - precision: 0.8917 - recall: 1.0000 - val_accuracy: 0.9615 - val_loss: 0.2397 - val_precision: 1.0000 - val_recall: 0.7857\n",
      "Epoch 38/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9966 - loss: 0.0266 - precision: 0.9823 - recall: 1.0000 - val_accuracy: 0.9615 - val_loss: 0.2705 - val_precision: 1.0000 - val_recall: 0.7857\n",
      "Epoch 39/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0300 - precision: 0.9563 - recall: 1.0000 - val_accuracy: 0.9487 - val_loss: 0.2016 - val_precision: 0.8846 - val_recall: 0.8214\n",
      "Epoch 40/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9974 - loss: 0.0134 - precision: 0.9867 - recall: 1.0000 - val_accuracy: 0.9615 - val_loss: 0.2034 - val_precision: 0.9583 - val_recall: 0.8214\n",
      "Validation Results:\n",
      "Loss: 0.1997, Accuracy: 0.9487, Recall: 0.8571, Precision: 0.8571\n",
      "Model saved as 'trained_hybrid_model.keras'\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# TRAIN CODE\n",
    "###############################\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter\n",
    "from geopy.distance import geodesic\n",
    "import warnings\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load and Combine CSV Files (with potholes column)\n",
    "# -----------------------------\n",
    "folder_path = r\"C:\\Users\\Rick Halder\\Desktop\\SpeedyCare\\Best Work\\Pothole_Non_Pothole\"  # <-- Update this path\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df[\"source_file\"] = os.path.basename(file)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "if df_list:\n",
    "    combined_data = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Combined DataFrame shape: {combined_data.shape}\")\n",
    "else:\n",
    "    raise ValueError(\"No CSV files were successfully read.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Helper Functions\n",
    "# -----------------------------\n",
    "def apply_kalman_filter(series, process_variance=1e-5, measurement_variance=1e-2):\n",
    "    n = len(series)\n",
    "    estimates = np.zeros(n)\n",
    "    error_estimate = 1.0\n",
    "    error_measure = measurement_variance\n",
    "    for i in range(n):\n",
    "        error_estimate += process_variance\n",
    "        kalman_gain = error_estimate / (error_estimate + error_measure)\n",
    "        estimate = series[i] if i == 0 else estimates[i-1]\n",
    "        estimate = estimate + kalman_gain * (series[i] - estimate)\n",
    "        error_estimate = (1 - kalman_gain) * error_estimate\n",
    "        estimates[i] = estimate\n",
    "    return estimates\n",
    "\n",
    "def z_thresh_detection(series, threshold=3):\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std()\n",
    "    z_scores = (series - mean_val) / std_val\n",
    "    anomaly_mask = (np.abs(z_scores) > threshold).astype(int)\n",
    "    return anomaly_mask, z_scores\n",
    "\n",
    "def isolation_forest_anomaly_detection(df, contamination=0.01):\n",
    "    features = df[['acc_magnitude', 'jerk']]\n",
    "    clf = IsolationForest(contamination=contamination, random_state=42)\n",
    "    preds = clf.fit_predict(features)\n",
    "    df['isof_anomaly'] = (preds == -1).astype(int)\n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_df(df, process_variance=1e-5, measurement_variance=1e-2, \n",
    "                           sg_window_length=21, sg_polyorder=3):\n",
    "    df = df.dropna(subset=['latitude', 'longitude'])\n",
    "    df = df[np.isfinite(df['latitude']) & np.isfinite(df['longitude'])]\n",
    "    if 'timestamp' in df.columns:\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        except Exception:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    else:\n",
    "        df['timestamp'] = pd.date_range(start='2025-01-01', periods=len(df), freq='s')\n",
    "    \n",
    "    # Rename columns if needed\n",
    "    rename_dict = {}\n",
    "    if 'accelerometerX' in df.columns:\n",
    "        rename_dict['accelerometerX'] = 'acc_x'\n",
    "    if 'accelerometerY' in df.columns:\n",
    "        rename_dict['accelerometerY'] = 'acc_y'\n",
    "    if 'accelerometerZ' in df.columns:\n",
    "        rename_dict['accelerometerZ'] = 'acc_z'\n",
    "    if 'gyroX' in df.columns:\n",
    "        rename_dict['gyroX'] = 'gyro_x'\n",
    "    if 'gyroY' in df.columns:\n",
    "        rename_dict['gyroY'] = 'gyro_y'\n",
    "    if 'gyroZ' in df.columns:\n",
    "        rename_dict['gyroZ'] = 'gyro_z'\n",
    "    if 'potholes' in df.columns:\n",
    "        rename_dict['potholes'] = 'pothole_label'\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    for col in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'pothole_label']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    df['acc_x_raw'] = df['acc_x'].copy()\n",
    "    sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    for col in sensor_cols:\n",
    "        filtered = apply_kalman_filter(df[col].values, process_variance, measurement_variance)\n",
    "        if len(filtered) >= sg_window_length:\n",
    "            df[col] = savgol_filter(filtered, window_length=sg_window_length, polyorder=sg_polyorder, mode='nearest')\n",
    "        else:\n",
    "            df[col] = filtered\n",
    "\n",
    "    df['acc_magnitude'] = np.linalg.norm(df[['acc_x', 'acc_y', 'acc_z']], axis=1)\n",
    "    df['gyro_magnitude'] = np.linalg.norm(df[['gyro_x', 'gyro_y', 'gyro_z']], axis=1)\n",
    "    time_diff = df['timestamp'].diff().dt.total_seconds().fillna(1)\n",
    "    df['jerk'] = np.gradient(df['acc_magnitude'], time_diff)\n",
    "    \n",
    "    distances = [0.0]\n",
    "    for i in range(1, len(df)):\n",
    "        prev_point = (df['latitude'].iloc[i-1], df['longitude'].iloc[i-1])\n",
    "        curr_point = (df['latitude'].iloc[i], df['longitude'].iloc[i])\n",
    "        distances.append(geodesic(prev_point, curr_point).meters)\n",
    "    df['distance'] = distances\n",
    "    \n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df = df.ffill().bfill().dropna(subset=['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'latitude', 'longitude'])\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    cols_to_scale = sensor_cols + ['acc_magnitude', 'gyro_magnitude', 'jerk']\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    df['z_anomaly'], df['z_score'] = z_thresh_detection(df['acc_magnitude'], threshold=3)\n",
    "    df['z_diff'] = df['z_score'].diff().fillna(0)\n",
    "    df['z_diff_anomaly'] = (np.abs(df['z_diff']) > 2).astype(int)\n",
    "    window_size = 50\n",
    "    df['rolling_mean'] = df['acc_magnitude'].rolling(window=window_size, min_periods=1).mean()\n",
    "    df['rolling_std'] = df['acc_magnitude'].rolling(window=window_size, min_periods=1).std().fillna(0)\n",
    "    df['std_anomaly'] = (np.abs(df['acc_magnitude'] - df['rolling_mean']) > 3 * (df['rolling_std'] + 1e-8)).astype(int)\n",
    "    threshold_low = 0.1\n",
    "    df['g_zero_flag'] = (df['acc_magnitude'] < threshold_low).astype(int)\n",
    "    \n",
    "    df = isolation_forest_anomaly_detection(df, contamination=0.01)\n",
    "    return df\n",
    "\n",
    "def extract_sliding_window_features(df, window_size=50, step=25):\n",
    "    feature_list = []\n",
    "    for start in range(0, len(df) - window_size + 1, step):\n",
    "        window = df.iloc[start:start+window_size]\n",
    "        center_idx = window_size // 2\n",
    "        features = {\n",
    "            'timestamp': window['timestamp'].iloc[center_idx],\n",
    "            'latitude': window['latitude'].iloc[center_idx],\n",
    "            'longitude': window['longitude'].iloc[center_idx],\n",
    "            'acc_magnitude': window['acc_magnitude'].mean(),\n",
    "            'acc_std': window['acc_magnitude'].std(),\n",
    "            'acc_min': window['acc_magnitude'].min(),\n",
    "            'acc_max': window['acc_magnitude'].max(),\n",
    "            'acc_skew': skew(window['acc_magnitude']),\n",
    "            'acc_kurtosis': kurtosis(window['acc_magnitude']),\n",
    "            'jerk_mean': window['jerk'].mean(),\n",
    "            'jerk_std': window['jerk'].std(),\n",
    "            'z_anomaly_rate': window['z_anomaly'].mean(),\n",
    "            'z_diff_rate': window['z_diff_anomaly'].mean(),\n",
    "            'std_anomaly_rate': window['std_anomaly'].mean(),\n",
    "            'g_zero_rate': window['g_zero_flag'].mean(),\n",
    "            'isof_anomaly_rate': window['isof_anomaly'].mean(),\n",
    "            'pothole_label': window['pothole_label'].iloc[center_idx]\n",
    "        }\n",
    "        feature_list.append(features)\n",
    "    return pd.DataFrame(feature_list)\n",
    "\n",
    "def create_sequences(features_df, feature_columns, sequence_length=10):\n",
    "    features_df = features_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    X_seq = []\n",
    "    for i in range(len(features_df) - sequence_length + 1):\n",
    "        seq = features_df.iloc[i:i+sequence_length]\n",
    "        X_seq.append(seq[feature_columns].values)\n",
    "    return np.array(X_seq), features_df\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Combined Tuning of Filtering Parameters (Dynamic)\n",
    "# -----------------------------\n",
    "# Use raw accelerometer data for tuning; prefer the 'acc_x_raw' column if available.\n",
    "if 'acc_x_raw' in combined_data.columns:\n",
    "    sensor_series_raw = combined_data['acc_x_raw'].values\n",
    "elif 'acc_x' in combined_data.columns:\n",
    "    sensor_series_raw = combined_data['acc_x'].values\n",
    "else:\n",
    "    np.random.seed(42)\n",
    "    t_sim = np.linspace(0, 10, 500)\n",
    "    sensor_series_raw = np.sin(t_sim) + np.random.normal(0, 0.2, t_sim.shape)\n",
    "\n",
    "def tuning_cost(params):\n",
    "    process_variance, measurement_variance, sg_window_length, sg_polyorder = params\n",
    "    sg_window_length = int(sg_window_length)\n",
    "    if sg_window_length % 2 == 0:\n",
    "        sg_window_length += 1\n",
    "    sg_polyorder = int(sg_polyorder)\n",
    "    if sg_polyorder >= sg_window_length:\n",
    "        return 1e6\n",
    "    kalman_filtered = apply_kalman_filter(sensor_series_raw, process_variance, measurement_variance)\n",
    "    sg_filtered = savgol_filter(kalman_filtered, window_length=sg_window_length, polyorder=sg_polyorder, mode='nearest')\n",
    "    residual = sensor_series_raw - sg_filtered\n",
    "    mae = np.mean(np.abs(residual))\n",
    "    nis = np.mean(((sensor_series_raw - sg_filtered) ** 2) / measurement_variance)\n",
    "    nis_error = np.abs(nis - 1)\n",
    "    return mae + nis_error\n",
    "\n",
    "space = [\n",
    "    Real(1e-6, 1e-3, prior='log-uniform', name='process_variance'),\n",
    "    Real(1e-3, 1e-1, prior='log-uniform', name='measurement_variance'),\n",
    "    Integer(7, 51, name='sg_window_length'),\n",
    "    Integer(1, 5, name='sg_polyorder')\n",
    "]\n",
    "\n",
    "result = gp_minimize(tuning_cost, space, n_calls=50, random_state=42)\n",
    "opt_process_variance    = result.x[0]\n",
    "opt_measurement_variance  = result.x[1]\n",
    "opt_sg_window_length    = int(result.x[2])\n",
    "if opt_sg_window_length % 2 == 0:\n",
    "    opt_sg_window_length += 1\n",
    "opt_sg_polyorder        = int(result.x[3])\n",
    "\n",
    "print(\"Optimized Filtering Parameters:\")\n",
    "print(\" Process Variance:\", opt_process_variance)\n",
    "print(\" Measurement Variance:\", opt_measurement_variance)\n",
    "print(\" SG Window Length:\", opt_sg_window_length)\n",
    "print(\" SG Polyorder:\", opt_sg_polyorder)\n",
    "\n",
    "# Save filtering parameters for use during inference.\n",
    "filtering_params = {\n",
    "    \"process_variance\": opt_process_variance,\n",
    "    \"measurement_variance\": opt_measurement_variance,\n",
    "    \"sg_window_length\": opt_sg_window_length,\n",
    "    \"sg_polyorder\": opt_sg_polyorder\n",
    "}\n",
    "with open(\"filtering_params.json\", \"w\") as f:\n",
    "    json.dump(filtering_params, f)\n",
    "print(\"Filtering parameters saved to filtering_params.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocess Data and Extract Features\n",
    "# -----------------------------\n",
    "combined_data_processed = load_and_preprocess_df(\n",
    "    combined_data,\n",
    "    process_variance=opt_process_variance,\n",
    "    measurement_variance=opt_measurement_variance,\n",
    "    sg_window_length=opt_sg_window_length,\n",
    "    sg_polyorder=opt_sg_polyorder\n",
    ")\n",
    "print(\"Processed data shape:\", combined_data_processed.shape)\n",
    "\n",
    "features_df = extract_sliding_window_features(combined_data_processed, window_size=50, step=25)\n",
    "print(\"Extracted features shape:\", features_df.shape)\n",
    "\n",
    "feature_columns = [\n",
    "    'acc_magnitude', 'acc_std', 'acc_min', 'acc_max', \n",
    "    'acc_skew', 'acc_kurtosis',\n",
    "    'jerk_mean', 'jerk_std',\n",
    "    'z_anomaly_rate', 'z_diff_rate', 'std_anomaly_rate', \n",
    "    'g_zero_rate', 'isof_anomaly_rate'\n",
    "]\n",
    "sequence_length = 10\n",
    "X_seq, features_df = create_sequences(features_df, feature_columns, sequence_length=sequence_length)\n",
    "print(\"Sequence shape:\", X_seq.shape, \"Labels shape:\", features_df.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create Labels and Train-Test Split\n",
    "# -----------------------------\n",
    "# Label each sequence as 1 if any window in the sequence shows a pothole.\n",
    "y_seq = []\n",
    "for i in range(len(features_df) - sequence_length + 1):\n",
    "    seq = features_df.iloc[i:i+sequence_length]\n",
    "    y_seq.append(1 if seq['pothole_label'].max() >= 1 else 0)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "class_weights_seq = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_seq), y=y_train_seq)\n",
    "class_weights_seq = dict(enumerate(class_weights_seq))\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Build and Train the Hybrid CNN-RNN Model\n",
    "# -----------------------------\n",
    "def build_hybrid_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall(name='recall'),\n",
    "                           tf.keras.metrics.Precision(name='precision')])\n",
    "    return model\n",
    "\n",
    "hybrid_model = build_hybrid_model(X_train_seq.shape[1:])\n",
    "print(\"Training Hybrid CNN-RNN Model...\")\n",
    "history = hybrid_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "    class_weight=class_weights_seq,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, acc, rec, prec = hybrid_model.evaluate(X_val_seq, y_val_seq, verbose=0)\n",
    "print(\"Validation Results:\")\n",
    "print(f\"Loss: {loss:.4f}, Accuracy: {acc:.4f}, Recall: {rec:.4f}, Precision: {prec:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Save the Trained Model in Native Keras Format\n",
    "# -----------------------------\n",
    "hybrid_model.save(\"trained_hybrid_model.keras\")\n",
    "print(\"Model saved as 'trained_hybrid_model.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5f715-2b03-4a40-b141-f1d257b7d58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
