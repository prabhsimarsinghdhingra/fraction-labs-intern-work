{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3c74f7f-82c8-4b17-bc70-8d8962f17ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded filtering parameters: {'process_variance': 0.0006715429033260829, 'measurement_variance': 0.05235353908705457, 'sg_window_length': 51, 'sg_polyorder': 1}\n",
      "Inference DataFrame shape: (5000, 12)\n",
      "Processed inference data shape: (5000, 26)\n",
      "Inference sequence shape: (190, 10, 13)\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted inference data saved as 'predicted_inference_data.csv'\n",
      "Folium map saved as 'predicted_potholes_map.html'\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# TEST / INFERENCE CODE\n",
    "###############################\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.signal import savgol_filter\n",
    "from geopy.distance import geodesic\n",
    "import warnings\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import folium\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Helper Functions\n",
    "# -----------------------------\n",
    "def apply_kalman_filter(series, process_variance=1e-5, measurement_variance=1e-2):\n",
    "    n = len(series)\n",
    "    estimates = np.zeros(n)\n",
    "    error_estimate = 1.0\n",
    "    error_measure = measurement_variance\n",
    "    for i in range(n):\n",
    "        error_estimate += process_variance\n",
    "        kalman_gain = error_estimate / (error_estimate + error_measure)\n",
    "        estimate = series[i] if i == 0 else estimates[i-1]\n",
    "        estimate = estimate + kalman_gain * (series[i] - estimate)\n",
    "        error_estimate = (1 - kalman_gain) * error_estimate\n",
    "        estimates[i] = estimate\n",
    "    return estimates\n",
    "\n",
    "def z_thresh_detection(series, threshold=3):\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std()\n",
    "    z_scores = (series - mean_val) / std_val\n",
    "    anomaly_mask = (np.abs(z_scores) > threshold).astype(int)\n",
    "    return anomaly_mask, z_scores\n",
    "\n",
    "def isolation_forest_anomaly_detection(df, contamination=0.01):\n",
    "    # Ensure there are no NaNs in the features used by IsolationForest\n",
    "    features = df[['acc_magnitude', 'jerk']].fillna(0)\n",
    "    clf = IsolationForest(contamination=contamination, random_state=42)\n",
    "    preds = clf.fit_predict(features)\n",
    "    df['isof_anomaly'] = (preds == -1).astype(int)\n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_df(df, process_variance=1e-5, measurement_variance=1e-2, \n",
    "                           sg_window_length=21, sg_polyorder=3):\n",
    "    df = df.dropna(subset=['latitude', 'longitude'])\n",
    "    df = df[np.isfinite(df['latitude']) & np.isfinite(df['longitude'])]\n",
    "    if 'timestamp' in df.columns:\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        except Exception:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    else:\n",
    "        df['timestamp'] = pd.date_range(start='2025-01-01', periods=len(df), freq='s')\n",
    "    \n",
    "    rename_dict = {}\n",
    "    if 'accelerometerX' in df.columns:\n",
    "        rename_dict['accelerometerX'] = 'acc_x'\n",
    "    if 'accelerometerY' in df.columns:\n",
    "        rename_dict['accelerometerY'] = 'acc_y'\n",
    "    if 'accelerometerZ' in df.columns:\n",
    "        rename_dict['accelerometerZ'] = 'acc_z'\n",
    "    if 'gyroX' in df.columns:\n",
    "        rename_dict['gyroX'] = 'gyro_x'\n",
    "    if 'gyroY' in df.columns:\n",
    "        rename_dict['gyroY'] = 'gyro_y'\n",
    "    if 'gyroZ' in df.columns:\n",
    "        rename_dict['gyroZ'] = 'gyro_z'\n",
    "    if 'potholes' in df.columns:\n",
    "        rename_dict['potholes'] = 'pothole_label'\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    for col in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'pothole_label']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    df['acc_x_raw'] = df['acc_x'].copy()\n",
    "    sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    for col in sensor_cols:\n",
    "        filtered = apply_kalman_filter(df[col].values, process_variance, measurement_variance)\n",
    "        if len(filtered) >= sg_window_length:\n",
    "            df[col] = savgol_filter(filtered, window_length=sg_window_length, polyorder=sg_polyorder, mode='nearest')\n",
    "        else:\n",
    "            df[col] = filtered\n",
    "\n",
    "    df['acc_magnitude'] = np.linalg.norm(df[['acc_x', 'acc_y', 'acc_z']], axis=1)\n",
    "    df['gyro_magnitude'] = np.linalg.norm(df[['gyro_x', 'gyro_y', 'gyro_z']], axis=1)\n",
    "    time_diff = df['timestamp'].diff().dt.total_seconds().fillna(1)\n",
    "    df['jerk'] = np.gradient(df['acc_magnitude'], time_diff)\n",
    "    \n",
    "    distances = [0.0]\n",
    "    for i in range(1, len(df)):\n",
    "        prev_point = (df['latitude'].iloc[i-1], df['longitude'].iloc[i-1])\n",
    "        curr_point = (df['latitude'].iloc[i], df['longitude'].iloc[i])\n",
    "        distances.append(geodesic(prev_point, curr_point).meters)\n",
    "    df['distance'] = distances\n",
    "    \n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df = df.ffill().bfill().dropna(subset=['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'latitude', 'longitude'])\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    cols_to_scale = sensor_cols + ['acc_magnitude', 'gyro_magnitude', 'jerk']\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    df['z_anomaly'], df['z_score'] = z_thresh_detection(df['acc_magnitude'], threshold=3)\n",
    "    df['z_diff'] = df['z_score'].diff().fillna(0)\n",
    "    df['z_diff_anomaly'] = (np.abs(df['z_diff']) > 2).astype(int)\n",
    "    window_size = 50\n",
    "    df['rolling_mean'] = df['acc_magnitude'].rolling(window=window_size, min_periods=1).mean()\n",
    "    df['rolling_std'] = df['acc_magnitude'].rolling(window=window_size, min_periods=1).std().fillna(0)\n",
    "    df['std_anomaly'] = (np.abs(df['acc_magnitude'] - df['rolling_mean']) > 3 * (df['rolling_std'] + 1e-8)).astype(int)\n",
    "    threshold_low = 0.1\n",
    "    df['g_zero_flag'] = (df['acc_magnitude'] < threshold_low).astype(int)\n",
    "    \n",
    "    # Ensure no NaN in the features used by IsolationForest\n",
    "    df[['acc_magnitude', 'jerk']] = df[['acc_magnitude', 'jerk']].fillna(0)\n",
    "    df = isolation_forest_anomaly_detection(df, contamination=0.01)\n",
    "    return df\n",
    "\n",
    "def extract_sliding_window_features(df, window_size=50, step=25):\n",
    "    feature_list = []\n",
    "    for start in range(0, len(df) - window_size + 1, step):\n",
    "        window = df.iloc[start:start+window_size]\n",
    "        center_idx = window_size // 2\n",
    "        features = {\n",
    "            'timestamp': window['timestamp'].iloc[center_idx],\n",
    "            'latitude': window['latitude'].iloc[center_idx],\n",
    "            'longitude': window['longitude'].iloc[center_idx],\n",
    "            'acc_magnitude': window['acc_magnitude'].mean(),\n",
    "            'acc_std': window['acc_magnitude'].std(),\n",
    "            'acc_min': window['acc_magnitude'].min(),\n",
    "            'acc_max': window['acc_magnitude'].max(),\n",
    "            'acc_skew': skew(window['acc_magnitude']),\n",
    "            'acc_kurtosis': kurtosis(window['acc_magnitude']),\n",
    "            'jerk_mean': window['jerk'].mean(),\n",
    "            'jerk_std': window['jerk'].std(),\n",
    "            'z_anomaly_rate': window['z_anomaly'].mean(),\n",
    "            'z_diff_rate': window['z_diff_anomaly'].mean(),\n",
    "            'std_anomaly_rate': window['std_anomaly'].mean(),\n",
    "            'g_zero_rate': window['g_zero_flag'].mean(),\n",
    "            'isof_anomaly_rate': window['isof_anomaly'].mean(),\n",
    "            'pothole_label': window['pothole_label'].iloc[center_idx]\n",
    "        }\n",
    "        feature_list.append(features)\n",
    "    return pd.DataFrame(feature_list)\n",
    "\n",
    "def create_sequences(features_df, feature_columns, sequence_length=10):\n",
    "    # If \"timestamp\" is missing, create a dummy timestamp column.\n",
    "    if 'timestamp' not in features_df.columns:\n",
    "        features_df['timestamp'] = pd.date_range(start='2025-01-01', periods=len(features_df), freq='s')\n",
    "    # Sort the DataFrame by timestamp\n",
    "    features_df = features_df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    X_seq = []\n",
    "    center_indices = []\n",
    "    for i in range(len(features_df) - sequence_length + 1):\n",
    "        seq = features_df.iloc[i:i+sequence_length]\n",
    "        X_seq.append(seq[feature_columns].values)\n",
    "        # Record the center index of this sequence.\n",
    "        center_indices.append(i + sequence_length // 2)\n",
    "    # Return the sequences and the subset of features corresponding to the center points.\n",
    "    return np.array(X_seq), features_df.iloc[center_indices].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load Dynamic Filtering Parameters\n",
    "# -----------------------------\n",
    "with open(r\"C:\\Users\\Rick Halder\\Desktop\\SpeedyCare\\Best Work\\filtering_params.json\", \"r\") as f:\n",
    "    filtering_params = json.load(f)\n",
    "\n",
    "opt_process_variance     = filtering_params[\"process_variance\"]\n",
    "opt_measurement_variance = filtering_params[\"measurement_variance\"]\n",
    "opt_sg_window_length     = filtering_params[\"sg_window_length\"]\n",
    "opt_sg_polyorder         = filtering_params[\"sg_polyorder\"]\n",
    "\n",
    "print(\"Loaded filtering parameters:\", filtering_params)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Inference Data (CSV files without the potholes column)\n",
    "# -----------------------------\n",
    "inference_folder_path = r\"C:\\Users\\Rick Halder\\Desktop\\SpeedyCare\\Best Work\\New folder\"  # <-- Update this path\n",
    "file_pattern = os.path.join(inference_folder_path, \"*.csv\")\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df[\"source_file\"] = os.path.basename(file)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "if df_list:\n",
    "    inference_data = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Inference DataFrame shape: {inference_data.shape}\")\n",
    "else:\n",
    "    raise ValueError(\"No CSV files were successfully read for inference.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocess Inference Data\n",
    "# -----------------------------\n",
    "inference_data_processed = load_and_preprocess_df(\n",
    "    inference_data,\n",
    "    process_variance=opt_process_variance,\n",
    "    measurement_variance=opt_measurement_variance,\n",
    "    sg_window_length=opt_sg_window_length,\n",
    "    sg_polyorder=opt_sg_polyorder\n",
    ")\n",
    "print(\"Processed inference data shape:\", inference_data_processed.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Extract Features and Create Sequences\n",
    "# -----------------------------\n",
    "features_df_infer = extract_sliding_window_features(inference_data_processed, window_size=50, step=25)\n",
    "feature_columns = [\n",
    "    'acc_magnitude', 'acc_std', 'acc_min', 'acc_max', \n",
    "    'acc_skew', 'acc_kurtosis',\n",
    "    'jerk_mean', 'jerk_std',\n",
    "    'z_anomaly_rate', 'z_diff_rate', 'std_anomaly_rate', \n",
    "    'g_zero_rate', 'isof_anomaly_rate'\n",
    "]\n",
    "X_seq_infer, features_df_infer = create_sequences(features_df_infer, feature_columns, sequence_length=10)\n",
    "print(\"Inference sequence shape:\", X_seq_infer.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Load Trained Model and Predict (using native Keras format)\n",
    "# -----------------------------\n",
    "trained_model = tf.keras.models.load_model(r\"C:\\Users\\Rick Halder\\Desktop\\SpeedyCare\\Best Work\\trained_hybrid_model.keras\")\n",
    "predictions = trained_model.predict(X_seq_infer)\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Merge Predictions into the Feature DataFrame\n",
    "# -----------------------------\n",
    "features_df_infer['Predicted_Pothole'] = predicted_labels\n",
    "\n",
    "# Save the predicted inference data to a CSV file.\n",
    "features_df_infer.to_csv(\"predicted_inference_data.csv\", index=False)\n",
    "print(\"Predicted inference data saved as 'predicted_inference_data.csv'\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Generate a Folium Map for Predicted Potholes\n",
    "# -----------------------------\n",
    "if features_df_infer['latitude'].notnull().any() and features_df_infer['longitude'].notnull().any():\n",
    "    center_lat = features_df_infer['latitude'].mean()\n",
    "    center_lon = features_df_infer['longitude'].mean()\n",
    "else:\n",
    "    center_lat, center_lon = 0, 0\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=13)\n",
    "\n",
    "for idx, row in features_df_infer.iterrows():\n",
    "    if row['Predicted_Pothole'] == 1:\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=5,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            fill_opacity=0.7,\n",
    "            popup=f\"Timestamp: {row['timestamp']}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "m.save(\"predicted_potholes_map.html\")\n",
    "print(\"Folium map saved as 'predicted_potholes_map.html'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092628e-eddc-4836-8a62-a7deb0304302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ba041-0f2c-44e1-81ea-48a26de93fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
